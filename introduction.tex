\firstsection{Introduction}

\maketitle

Over fifty years ago, John Tukey published ``The Future of Data Analysis''
in which he defined a remarkably farsighted agenda for the future of
data analysis and statistical computing. His vision has been realized
through environments like S and R, that started a revolution by
integrating statistical graphics, computation, and robust techniques
for practical data modeling. Every day, many thousands of data scientists
and others specialists benefit from this astounding vision.

The field has now moved beyond the boundaries of what Tukey originally
foresaw. Processing and networking capabilities today far exceed
what was barely imagined fifty years ago. Inexpensive, scalable
virtualized systems allow analysts to routinely solve problems
involving millions or even billions of data values. Networking
also enables both collaboration and the automation of complex
tasks and processes in ways that were not considered even ten years ago.
We turn our attention to this opportunity to use computation
to support, not only individuals, but entire teams and their
work within larger organizations.

As a starting point, consider the role of a data science team within
a business or technical organization today.
Analysts work in loose or tight collaboration with colleagues and
domain experts.
Project teams vary in size, from just a few people to dozens or
more, even within one project's duration. Assignments are often
broad, and include tasks such as problem identification,
data wrangling, modeling, analysis, visualization, summarization,
presentation and interpretation of results, and recommending
actions to help clients to realize the benefits of the analysis.
Eventually, knowledge or working prototypes created by data science teams
may be transferred to other organizations to employ them in production.
There are many details to these tasks. For example, data wrangling can involve
finding data, understanding its syntax and semantics, assessing data quality,
performing normalization and data quality remediation, and making the data
available to other tasks that will follow.

Visual analytics depends greatly on communication and collaboration.
At almost every step, detailed knowledge about data, code and tasks
is shared by collaborators. 
Further, data scientists are increasingly asked to work more closely
with business or domain specialists who may be less technically oriented.
Thus, data scientists and developers are being asked to become
very broad, and integrate work across the spectrum.

Interviews in {\it Data Scientists at Work} by Sebastian Gutierrez illuminate
the teamwork and sharing in the current practice of visual analytics in industry.
We particularly appreciated the remarks by Jonathan Lenaghan about the potential
for continuous integration and publication.

% \begin{addmargin}[1em]{1em}  % left and right margin change
% part1.txt
Erin Shellman [of Nordstrom] described the benefits of
continuous deployment of experiments, and sharing knowledge
through software and data analysis artifacts.
``Finally, prototyping our products so that internal
customers can use them early on has been crucial for
our success. It doesn't even have to be something
fancy. For instance, our recommendations preview
tool doesn't have a particularly interesting
visualization, but its enough to show the result of our
algorithms. Now we can shoot off a URL to internal
customers and it allows them to sit at their desk and
understand the behavior of our product and
experiment with it, and provide feedback way before
we're talking about getting it into production. This
has been super helpful and has been a really great
way to get people excited about what we're working
on...
%Building and maintaining those relationships are
%just like anything else-- you always have to be
%working on them. Nurturing those relationships are
%part of the job, and if you leave them unattended,
%they might not be there later.
%
% part2.txt
%Gutierrez: How do you share the knowledge you are
%building with others?
We’re obsessed with Confluence from
Atlassian, and the Data Lab has a very active
Confluence space. Recommendo is fully documented
on Confluence, so anyone in the company could learn
how it’s built, where to find it, and how to use it.
We also share exploratory analyses and reports on
Confluence so that we can still exchange knowledge
even if the work didn't make it into a larger project.''

% part3.txt
John Foreman commented on the scope of the work in
data science teams.
``I lead the data science team at
MailChimp, and I like to get my hands dirty too.
Some of the big pieces of my day involve working
with my team to take stock of current projects and
figure out where to go next, doing my own work and
prototyping things-- I do projects just like my peers
do and then also my talking with other teams,
talking with management, and planning for the
future.
On our team, we've got different folks facing different
% part4.txt
kinds of projects. We've got one person who really
owns compliance and looks at our compliance
processes. We've got another data scientist who
focuses on more of the user experience side of the
house and understanding our customers. I help them
and others as is needed while I do the three things
mentioned earlier-- build data products, be a
translator, and have conversations with the data
science community.''

% part5.txt
Jonathan Lenaghan from PlaceIQ discussed to the potential
for continuous deployment in data science.
``If you look at dev ops right now, they have things
such as continuous integration, continuous build,
automated testing, and test harnesses-- a1l of which
map very well from the dev ops world to the data ops
(a phrase I stole from Red Monk) world very easily. I
think this is a very powerful notion. It is important to
have testing frameworks for all of your data, so that if
you make a code change, you can go back and test all
of your data.''

% part6.txt
Anna Smith from Bitly spoke about the benefits of access to
a rich environment of shared data and code.
``In addition to Bitly's extraordinary data sets,
I was given all the resources that I needed. These were
resources that I’d only heard about but had never
seen. I went from asking what a Hadoop cluster was,
as my school didn't have one, to being able to work
with one. Now I could play with one to figure out how
to make my work better and how to change my
algorithms so they were cleaner and ran faster on
Hadoop. The whole transition was eye opening. Not
only that, I could also look at other people’s code and
play with their code and data sets as well. It was
fantastic and I was convinced that working with data was my thing.''
% \end{addmargin}

% (Hiring ``full stack developers'' to perform integration seems to be a trend, but does not achieve the potential benefits of specialization and division of labor in larger teams, and is not a practice to be encouraged.)

Unfortunately, processes and technology that support visual analytics today
work are fragmented.
Knowledge is shared through informal conversations, email online messages,
meetings, phone calls, within code itself, in wikis, in project documents and
by other means. Results of experiments are often shared first by copying static
output, later, by holding meetings that lead to moving or even rewriting
entire programs.

This disjointed, inconsistent process inhibits the use of tools to bridge
the many gaps and to help people find resources.
For example, even when code is shared, we still not may know where to
find examples that show how to use it.
When we find a useful data source, we don't know what packages or
other data sources usually go along with it, what problems with the data
others have already discovered and corrected, or where to obtain online feeds
to get updates.

This situation definitely affects how well data science teams work.
It limits our ability to share work and to rapidly deploy and refine results.
We have seen this in our lab environment.
Other authors report similar findings.

Some symptoms of the current situation are:
\begin{itemize}
\item It is hard to find data, metadata, and knowledge about them.
\item Most coordination is done through meetings, whose content is not related to other artifacts and may not even be stored.
\item Production deployment requires porting code to a different environment or even completely rewriting it, thwarting continuous release.
\item Many tools adopt a standalone or single developer perspective,
not suited to collaboration and web deployment. 
\item Analysts find insufficient support for tracing events or issues from production back to the EDA process.
\end{itemize}

In view of the opportunity to improve this situation,
we created a software environment that supports the end to end
visual analytics process for individuals and teams.
This environment knits together some familiar tools, and 
provides new features to find data and code; create experimental workbooks;
run experiments; annotate and deploy experiments as end-user websites or
as reusable, callable services; and to share, search and recommend these artifacts. The artifacts are stored in a version
control system that provides a common workspace, as well as needed control
and isolation between stable and experimental versions of code and other
resources. 

Over the past two years, we prototyped this environment and deployed it
in a production community of data scientists, business analysts and other colleagues.
The platform today has over 200 active accounts, and several dozen people have forked
and customized code in the system. Most of the active users are members of
the statistics research department of our lab, but some others are data
scientists and other collaborators in business units. \stephen{I hope this is right.}

A key point is that, for the most part, the improved capabilities are
available by default, without much explicit intervention on the part of
data scientists and other customers. Visual analytics experiments are
conveniently shared and turned into production websites, without moving
or porting code. All the published artifacts in the system can be
searched immediately. Recommendation is as easy as clicking a star
on a useful workbook.

The high level architecture of the prototype system
is shown in figure~\ref{fig:high-level-architecture}.
We chose R as the foundation for our prototype because it is already
the dominant statistical computing language in our lab.
R also has many useful packages for data analysis and visualization,
and the core system and its packages are open source that can be modified to
support research. 

There is much previous work that inspired or influenced ours.
Indeed, many studies have addressed the requirements for future visual
analytics environments.
Previous work that we learned from includes:

{\bf Vistrails} as a landmark prototype of a scientific workflow
and provenance management system. This prototype helped the community
to understand the value of capturing aspects of the processes that
surround data analysis experiments and tools, including detailed
history, collaboration, and deployment.

{\bf iPython notebooks (Jupyter)} are the closest work we are aware of.
The goal of this project is to provide shared notebooks and a remote
execution environment for interpreted scripts. \stephen{Are there 
significant differences involving versioning, provenance, annotation
and other broad process support?}

{\bf RStudio, RShiny} and other packages based on R for creating and
publishing web content such as knitR, Rpubs, rCharts, and gg2v.
These tools are not collaborative, but they are aimed at better
graphics and web applications.

{\bf bl.ocks, jsfiddle, plotly} and other web services for sharing code
and demos.

{ManyEyes} is the best-known original work showing the potential for
integrating data visualization with social media.

Further afield, {\bf Electronic Lab Notebooks} are applications for organizing
and sharing data from scientific lab experiments\cite{Rubacha:2011:ELN}.
In a sense, we hope to adapt and extend this concept to the work of
visual analytics teams.

From a more principled or theoretical perspective, visual analytics goals
or objectives for emerging systems, described in previous work, closely match
the goals of our prototype. Adopting the terminology of X Y and Z, we noted
the following.

\begin{enumerate}

\item Technology transfer.
In most organizations, development of analyses and visualizations 
s done by \emph{hackers} and \emph{scripters} (adopting Kandel et al.'s
terminology~\cite{Kandel:2012:EDA}). \emph{Application users} 
gain the benefits of the tools developed by hackers and scripters.
Generally, the connection between these two worlds is made by IT staff,
who port or even rewrite code so it can run as a stable production service.
In an environment where business needs can change rapidly, this
process does not scale. Our objective is to merge the worlds
of EDA and production.

\item Coexistence. In the current data science ecosystem, the
isolation of exploratory visualization and data analysis
environments hinders wider adoption of modern techniques from each.
The richness of interactive visualization tools is still somewhat
separated from the power of statistical programming environments.
There is an opportunity to provide a framework so that developments
on each side are more easily adopted by the other, and made available
in production services.

\item Discoverability. A chronic complaint of data analysis teams is
repeated work (``How can I connect to database X?'', ``How do you get
clean data from column Y from feed Z?'') This work arises from lack of
communication about previously solved subproblems. Our goal is encourage
and support transparency of work between team members.

\end{enumerate}
