\firstsection{Introduction}

\maketitle

Over fifty years ago, John Tukey published ``The Future of Data Analysis''
in which he defined a remarkably prescient agenda for the future of
data analysis and statistical computing. His vision has been realized
through environments like S and R, that have encouraged the adoption of
statistical graphics, and robust techniques for practical data modeling.
The software people use today has fulfilled his vision for the practice
of data analysis by experts.

The field has now moved beyond the boundaries of what Tukey originally
foresaw. Both computational and networking capabilities today far exceed
what could have been imagined fifty years ago. Cheap, highly parallel
virtualized systems allow analysts to solve problems at a scale that
was once difficult to conceive. But platforms today also enable collaboration
and support complex tasks and work processes in ways that were also
hard to imagine in the past. We turn our attention to this opportunity.

Consider the role of a data science team within an organization today.
Analysts work in loose or tight collaboration with colleagues.
Project teams vary in size, from just a few people to dozens or
more, even within one project's duration. Tasks are broad, including problem
identification, data wrangling, visualization, modeling and analysis,
interpreting, explaining and advocating the results, and often helping
clients to realize the benefits of the work in ongoing processes.
There are many details to these tasks.
For example, data wrangling can involve finding data, figuring out its
syntax and semantics, assessing data quality, performing normalization and
data quality remediation, and making the data available for other tasks
to follow.

Visual analytics in practice depends greatly on communication and collaboration.
At almost every step, detailed knowledge about data, code and tasks
is shared by collaborators. 
Further, data scientists are increasingly asked to work more closely
with business people who are domain specialists and less technically oriented.
Thus, data scientists and developers are being asked to become
very broad, and integrate work across the spectrum. (Hiring ``full stack
developers'' to perform integration seems to be a trend, but does not achieve
the potential benefits of specialization and division of labor in larger
teams, and is not a practice to be encouraged.)
This pushes the boundaries for communication and collaboration even further.
Unfortunately, processes and technology that support this work are fragmented.
Knowledge is shared through informal conversations, email online messages,
meetings, phone calls, within code itself, in wikis, in project documents and
by other means. Results of experiments are often shared first by copying static
output, later, by holding meetings that lead to moving or even rewriting
entire programs.

This disjointed process inhibits the use of tools to bridge
the many gaps and to help people find resources.
For example, when code is shared, we still not may know where to find examples
that show how to use it. stephen{Why did I shift to first person?}
When we find a useful data source, we don't know what packages or
other data sources usually go along with it, or what problems with the data
others have already discovered and corrected, or where to obtain online feeds
to get updates of the data.

The current situation affects how well data science teams work. It limits
the ability to share work and to rapidly deploy and refine results.
We have seen this in our lab environment.
Other authors report similar findings.

Some symptoms of the current situation are:
\begin{itemize}
\item It is hard to find data, metadata, and previously hard-won knowledge about it.
\item Most coordination is done through meetings, whose content is not related to other artifacts and may not even be stored.
\item Production deployment requires porting code to a different environment or even completely rewriting it, thwarting continuous release.
\item Many tools adopt a standalone or single developer perspective,
not suited to collaboration and web deployment. 
\item Analysts find insufficient support for tracing events or issues from production back to the EDA process
\end{itemize}

In view of the opportunity to improve this situation,
we created a software environment that supports the end to end
visual analytics process for individuals and teams.
This environment knits together some familiar tools, and it
provides new features to find data and code; create experimental workbooks;
run experiments; annotate and deploy experiments as end-user websites or
as reusable, callable services; and to share, search and recommend these artifacts. The artifacts are stored in a version
control system that provides a common workspace, as well as needed control
and isolation between stable and experimental versions of code and other
resources. 

Over the past two years, we prototyped this environment and deployed it
in a production community of data scientists, business analysts and other colleagues.
The platform today has over 200 active accounts, and several dozen people have forked
and customized code in the system.

A key point is that, for the most part, the improved capabilities are
available by default, without much explicit intervention on the part of
data scientists and other customers. Visual analytics experiments are
conveniently shared and turned into production websites, without moving
or porting code. All the published artifacts in the system can be
searched immediately. Recommendation is as easy as clicking a star
on a useful workbook.

The high level architecture of the prototype system
is shown in figure~\ref{fig:high-level-architecture}.
We chose R as the foundation for our prototype because it is already
the dominant statistical computing language in our lab.
R also has many useful packages for data analysis and visualization,
and the core system and its packages are open source that can be modified to
support research. 

There is much previous work that inspired or influenced ours. Some
software prototypes that we learned from include:

{\bf Vistrails} as an important prototype of a scientific workflow
and provenance management system. The prototype helped the community
to understand the value of capturing aspects of the processes that
surround data analysis experiments and tools, including detailed
history, collaboration, and deployment.

{\bf iPython notebooks (Jupyter)} are the closest work we are aware of.
Of particular note, 

{\bf RStudio, RShiny} and other packages based on R for creating and
publishing web content such as knitR, Rpubs, rCharts, and gg2v.

{\bf bl.ocks, jsfiddle, plotly} and other web services for sharing code
and demos.

{ManyEyes} is the best-known foundational work showing the potential for
integrating data visualization with social media.

Further afield, {\bf Electronic Lab Notebooks} are applications for organizing
data from lab experiments and sharing it within research teams \cite{Rubacha:ELN:2011}. In a sense, we hope to adapt and extend this concept to the work of
visual analytics teams.

From a more principled or theoretical perspective, visual analytics goals
or objectives for emerging systems, described in previous work, closely match
the goals of our prototype.  Adopting the terminology of X Y and Z, we noted
the following.

\begin{enumerate}

\item Technology transfer.
In most organizations, development of analyses and visualizations 
s done by \emph{hackers} and \emph{scripters} (using Kandel et al.'s
terminology~\cite{Kandel:2012:EDA}). \emph{Application users} 
gain the benefits of the tools developed by hackers and scripters.
Generally, the connection between these two worlds is made by IT staff,
who port or even rewrite code so it can run as a stable production service.
In an environment where business needs can change rapidly, this
process does not scale. Our objective is to connect the two worlds
of EDA and production.

\item Coexistence. In the current data science ecosystem, the
isolation of exploratory visualization and data analysis
environments hinders wider adoption of modern techniques.
The richness of interactive visualization tools is still somewhat
separated from the power of statistical programming environments.
There is an opportunity to provide a framework so that developments
on each side are more easily adopted by the other, and made available
in production services.

\item Discoverability. A chronic complaint of data analysis teams is
repeated work (``How can I connect to database X?'', ``How do you get
clean data from column Y from feed Z?'')  This work arises from lack of
communication about previously solved subproblems.  Our goal is encourage
and support transparency of work between team members.

\end{enumerate}

\stephen{We could now tell them what we're going to tell them here, but I'm
not sure that's a good approach - unless we feel the reader is having
trouble following the path we are laying out, it seems to disrupt the
flow of the discussion.}
